{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lstAo3s6EfO"
      },
      "source": [
        "# NLTK EXCERSIES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYaUuew_6EfU"
      },
      "source": [
        "# Tokenization:\n",
        "Tokenization is the process of breaking a text into individual words, phrases, symbols, or other meaningful elements called tokens. Tokens are the basic units of text that carry meaning and form the building blocks for further analysis. The tokenization process involves segmenting the text based on certain rules, such as separating words by spaces or punctuation marks. Tokenization helps to structure the text into manageable units, enabling subsequent analysis and processing tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQPceIOj6dm7",
        "outputId": "6781a2ca-0524-40dd-eaa2-b108926ec872"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HKJZqdM6EfV",
        "outputId": "16dc61de-84f2-4a07-bf42-92f7a4153dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'ladies', 'in', 'Arewa', 'Data', 'Science', 'Academy', 'are', 'among', 'the', 'best', 'data', 'scientists', 'in', 'Arewa', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "sentence = \"The ladies in Arewa Data Science Academy are among the best data scientists in Arewa.\"\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Print the tokens\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCjxe0o46EfY"
      },
      "source": [
        "# TAGGING:\n",
        "\n",
        "Part-of-speech tagging, POS-tagging, or simply tagging is the process of classifying words into their parts of speech and labeling them accordingly. Word classes and lexical categories are other terms for parts of speech. A tagset is a collection of tags used for a specific task. This chapter focuses on utilizing tags and automatically tagging text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxNhFsMm6EfZ",
        "outputId": "9b3b8949-99b1-44a6-9b90-12aa2fa34576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: The\tTag: DT\n",
            "Token: ladies\tTag: NNS\n",
            "Token: in\tTag: IN\n",
            "Token: Arewa\tTag: NNP\n",
            "Token: Data\tTag: NNP\n",
            "Token: Science\tTag: NNP\n",
            "Token: Academy\tTag: NNP\n",
            "Token: are\tTag: VBP\n",
            "Token: among\tTag: IN\n",
            "Token: the\tTag: DT\n",
            "Token: best\tTag: JJS\n",
            "Token: data\tTag: NN\n",
            "Token: scientists\tTag: NNS\n",
            "Token: in\tTag: IN\n",
            "Token: Arewa\tTag: NNP\n",
            "Token: .\tTag: .\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# POS Tagging\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "# Print the tagged tokens\n",
        "for token, tag in tagged_tokens:\n",
        "    print(f\"Token: {token}\\tTag: {tag}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0HacPuW6EfZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}